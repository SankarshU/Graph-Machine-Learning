{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9896f3d-60ab-4203-94aa-f496f88a4952",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd2ab2-ecc5-45da-9af9-af6406579f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GACSE(nn.Module):\n",
    "    def __init__(self, n_user, n_item, norm_adj, args):\n",
    "        super(GACSE, self).__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.device = args.device\n",
    "        self.emb_size = args.embed_size\n",
    "        self.batch_size = args.batch_size\n",
    "        self.node_dropout = args.node_dropout[0]\n",
    "        self.mess_dropout = args.mess_dropout\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        # *************************************************\n",
    "        self.weighted_sample_num = args.weighted_sample_num\n",
    "        self.n_heads = 3\n",
    "        self.neighbors_num = [1] + eval(args.neighbors_num)\n",
    "        self.node_dropout_flag = args.node_dropout_flag\n",
    "        #**************************************************\n",
    "\n",
    "        self.norm_adj = norm_adj\n",
    "\n",
    "        self.layers = eval(args.layer_size)\n",
    "        self.decay = eval(args.regs)[0]\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Init the weight of user-item.\n",
    "        \"\"\"\n",
    "        self.embedding_dict, self.weight_dict = self.init_weight()\n",
    "        # print(\"embedding_dict\",self.embedding_dict, \"weight_dict\",self.weight_dict)\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Get sparse adj.\n",
    "        \"\"\"\n",
    "        self.sparse_norm_adj = self._convert_sp_mat_to_sp_tensor(self.norm_adj)\n",
    "\n",
    "        # self.ua_embeddings, self.ia_embeddings = self._trans_all_embed(self.embedding_dict['user_emb'], self.embedding_dict['item_emb'])\n",
    "        # print(\"initial ua_embeddings\", self.ua_embeddings, \"initial ia_embeddings\", self.ia_embeddings)\n",
    "\n",
    "    def init_weight(self):\n",
    "        # xavier init\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(initializer(torch.empty(self.n_user,\n",
    "                                                 self.emb_size))),\n",
    "            'item_emb': nn.Parameter(initializer(torch.empty(self.n_item,\n",
    "                                                 self.emb_size))),\n",
    "            'cse_user_emb': nn.Parameter(initializer(torch.empty(self.n_user,\n",
    "                                                 self.emb_size))),\n",
    "            'cse_item_emb': nn.Parameter(initializer(torch.empty(self.n_item,\n",
    "                                                 self.emb_size)))\n",
    "        })\n",
    "        \n",
    "        tar=np.load(lgcn_data1+'ttarr.npy')\n",
    "        war=np.load(lgcn_data1+'woarr.npy')\n",
    "#         #embedding_user.weight.data.copy_(torch.from_numpy(tar))\n",
    "#         #embedding_item.weight.data.copy_(torch.from_numpy(war))         \n",
    "#         embedding_dict = nn.ParameterDict({\n",
    "#             'user_emb': embedding_user,\n",
    "#             'item_emb': embedding_item \n",
    "#         })\n",
    "        \n",
    "#         #embedding_dict['user_emb'].update.weight.data.copy_(torch.from_numpy(tar))\n",
    "        embedding_dict['user_emb'].data = torch.FloatTensor(tar)\n",
    "        embedding_dict['item_emb'].data = torch.FloatTensor(war)\n",
    "        embedding_dict['user_emb'].requires_grad = False\n",
    "        embedding_dict['item_emb'].requires_grad = False\n",
    "        \n",
    "        weight_dict = nn.ParameterDict()\n",
    "        self.layers_list = [self.emb_size] + self.layers\n",
    "        \n",
    "        weight_dict.update({'W_trans': nn.Parameter(initializer(torch.empty(self.emb_size,\n",
    "                                                                    self.layers_list[0])))})\n",
    "        weight_dict.update({'b_trans': nn.Parameter(initializer(torch.empty(1, self.layers_list[0])))})\n",
    "\n",
    "        weight_dict.update({'W_att': nn.Parameter(initializer(torch.empty(self.emb_size*2,\n",
    "                                                                    self.layers_list[0])))})\n",
    "        weight_dict.update({'V_att': nn.Parameter(initializer(torch.empty(self.layers_list[0],1)))})\n",
    "\n",
    "        weight_dict.update({'W_att_l': nn.Parameter(initializer(torch.empty(self.layers_list[0],\n",
    "                                                                    self.layers_list[0])))})\n",
    "        weight_dict.update({'W_att_r': nn.Parameter(initializer(torch.empty(self.layers_list[0],\n",
    "                                                                    self.layers_list[0])))})                                                            \n",
    "\n",
    "        return embedding_dict, weight_dict\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo()\n",
    "        i = torch.LongTensor([coo.row, coo.col])\n",
    "        v = torch.from_numpy(coo.data).float()\n",
    "        return torch.sparse.FloatTensor(i, v, coo.shape)\n",
    "\n",
    "    def sparse_dropout(self, x, rate, noise_shape):\n",
    "        random_tensor = 1 - rate\n",
    "        random_tensor += torch.rand(noise_shape).to(x.device)\n",
    "        dropout_mask = torch.floor(random_tensor).type(torch.bool)\n",
    "        i = x._indices()\n",
    "        v = x._values()\n",
    "\n",
    "        i = i[:, dropout_mask]\n",
    "        v = v[dropout_mask]\n",
    "\n",
    "        out = torch.sparse.FloatTensor(i, v, x.shape).to(x.device)\n",
    "        return out * (1. / (1 - rate))\n",
    "\n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "        pos_neg_scores = torch.sum(torch.mul(pos_items, neg_items), axis=1)\n",
    "#         maxi = nn.LogSigmoid()(pos_scores - neg_scores)\n",
    "\n",
    "#         mf_loss = -1 * torch.mean(maxi)\n",
    "#         print(pos_neg_scores)\n",
    "\n",
    "        mf_loss = torch.mean(nn.Softplus()((-1*(pos_scores - neg_scores  - nn.ReLU()(pos_neg_scores)))))\n",
    "\n",
    "        # cul regularizer\n",
    "        regularizer = (torch.norm(users) ** 2\n",
    "                       + torch.norm(pos_items) ** 2\n",
    "                       + torch.norm(neg_items) ** 2) / 2\n",
    "        emb_loss = self.decay * regularizer / self.batch_size\n",
    "\n",
    "        return mf_loss + emb_loss, mf_loss, emb_loss\n",
    "\n",
    "    def rating(self, u_g_embeddings, pos_i_g_embeddings):\n",
    "        return torch.matmul(u_g_embeddings, pos_i_g_embeddings.t())\n",
    "\n",
    "    def forward(self, users, pos_items_u, neg_items_u, users_pos, users_neg, pos_items_pos, pos_items_neg, neg_items_pos, neg_items_neg, ua_embeddings, ia_embeddings):\n",
    "\n",
    "        u_g_embeddings = []\n",
    "        pos_i_g_embeddings = []\n",
    "        neg_i_g_embeddings = []\n",
    "        for i in range(len(self.neighbors_num) - 1):\n",
    "            u_x, u_neib = ua_embeddings[users[i], :], ia_embeddings[users[i+1], :]\n",
    "            pos_iu_x, pos_iu_neib = ia_embeddings[pos_items_u[i], :], ua_embeddings[pos_items_u[i+1], :]\n",
    "            \n",
    "            u_g = self._agg(u_x, u_neib, self.neighbors_num[i+1])\n",
    "            # print(\"u_g\",u_g.device)\n",
    "            pos_i_g = self._agg(pos_iu_x, pos_iu_neib, self.neighbors_num[i+1])\n",
    "            # print(\"pos_i_g\",pos_i_g.device)\n",
    "            u_g_embeddings.append(u_g)\n",
    "            pos_i_g_embeddings.append(pos_i_g)\n",
    "            if len(neg_items_u)>0:\n",
    "                neg_iu_x, neg_iu_neib = ia_embeddings[neg_items_u[i], :], ua_embeddings[neg_items_u[i+1], :]\n",
    "                neg_i_g = self._agg(neg_iu_x, neg_iu_neib, self.neighbors_num[i+1]) \n",
    "                # print(\"neg_i_g\",neg_i_g.device)\n",
    "                \n",
    "                neg_i_g_embeddings.append(neg_i_g)\n",
    "\n",
    "\n",
    "        u_cse = self.embedding_dict['user_emb'][users[0], :]\n",
    "        u_p_cse = self.embedding_dict['cse_user_emb'][users_pos, :]\n",
    "        u_n_cse = self.embedding_dict['cse_user_emb'][users_neg, :]\n",
    "\n",
    "        pos_i_cse = self.embedding_dict['item_emb'][pos_items_u[0], :]\n",
    "        pos_i_pos_cse = self.embedding_dict['cse_item_emb'][pos_items_pos, :]\n",
    "        pos_i_neg_cse = self.embedding_dict['cse_item_emb'][pos_items_neg, :]\n",
    "\n",
    "        u_g_embeddings += [u_cse]\n",
    "        pos_i_g_embeddings += [pos_i_cse]\n",
    "        u_g_embeddings = torch.cat(u_g_embeddings, 1)\n",
    "        pos_i_g_embeddings = torch.cat(pos_i_g_embeddings, 1)\n",
    "        \n",
    "        if len(neg_items_u)>0:\n",
    "            neg_i_cse = self.embedding_dict['item_emb'][neg_items_u[0], :]\n",
    "            neg_i_pos_cse = self.embedding_dict['cse_item_emb'][neg_items_pos, :]\n",
    "            neg_i_neg_cse = self.embedding_dict['cse_item_emb'][neg_items_neg, :]\n",
    "            neg_i_g_embeddings += [neg_i_cse]\n",
    "            neg_i_g_embeddings = torch.cat(neg_i_g_embeddings, 1)\n",
    "            return u_g_embeddings, pos_i_g_embeddings, neg_i_g_embeddings, u_cse, u_p_cse, u_n_cse, pos_i_cse, pos_i_pos_cse, pos_i_neg_cse, neg_i_cse, neg_i_pos_cse, neg_i_neg_cse\n",
    "        \n",
    "        else:\n",
    "            return u_g_embeddings, pos_i_g_embeddings,  torch.empty(1),  torch.empty(1),  torch.empty(1),  torch.empty(1),  torch.empty(1),  torch.empty(1),  torch.empty(1),  torch.empty(1),  torch.empty(1),  torch.empty(1)\n",
    "        \n",
    "    def _trans_all_embed(self):\n",
    "        A_hat = self.sparse_dropout(self.sparse_norm_adj,\n",
    "                                    self.node_dropout,\n",
    "                                    self.sparse_norm_adj._nnz()) if self.node_dropout_flag else self.sparse_norm_adj\n",
    "        # print(\"A_hat\",A_hat.device)\n",
    "        local_embeddings = torch.cat([self.embedding_dict['user_emb'],\n",
    "                                    self.embedding_dict['item_emb']], 0).to(self.device)\n",
    "        # print(\"local_embeddings\",local_embeddings.device)\n",
    "        \n",
    "        message_embeddings = torch.sparse.mm(A_hat, local_embeddings)\n",
    "        # print(\"message_embeddings\",message_embeddings.device)\n",
    "        # print(\"message_embeddings\",self.weight_dict['W_trans'].device, self.weight_dict['b_trans'].device)\n",
    "        message_embeddings = nn.LeakyReLU(negative_slope=0.2)(torch.matmul(message_embeddings, self.weight_dict['W_trans'].to(self.device)) \\\n",
    "                                             + self.weight_dict['b_trans'].to(self.device))\n",
    "        \n",
    "        users_embedings = message_embeddings[:self.n_user, :]\n",
    "        item_embeddings = message_embeddings[self.n_user:, :]\n",
    "\n",
    "        return users_embedings, item_embeddings\n",
    "\n",
    "\n",
    "    def _agg(self, x, neighbors, num_neighbors):\n",
    "        x_att = torch.reshape(x, (-1, 1, self.layers_list[0]))\n",
    "#         print(x_att.shape)\n",
    "        neighbors_att = torch.reshape(neighbors, (-1, num_neighbors, self.layers_list[0]))\n",
    "#         print(neighbors_att.shape)\n",
    "        # neighbors_att = tf.transpose(neighbors_att, perm=[0,2,1])\n",
    "        x_att = x_att.repeat(1, num_neighbors,1)\n",
    "#         x_att = torch.repeat_interleave(x_att, torch.tensor([1, num_neighbors,1]), dim=1)\n",
    "#         print(x_att.shape)\n",
    "        all_emb = torch.cat([x_att, neighbors_att], 2)\n",
    "        all_emb = torch.reshape(all_emb, (-1, self.layers_list[0]*2))\n",
    "\n",
    "        scores = nn.Tanh()(torch.matmul(all_emb, self.weight_dict['W_att'].to(self.device)))\n",
    "        scores = torch.matmul(scores, self.weight_dict['V_att'].to(self.device))\n",
    "        scores = torch.reshape(scores, (-1, 1, num_neighbors))\n",
    "        scores = nn.Softmax(dim=2)(scores)\n",
    "\n",
    "        # neighbors_att = tf.transpose(neighbors_att, perm=[0,2,1])\n",
    "        agg_neib = torch.squeeze(torch.matmul(scores, neighbors_att))\n",
    "\n",
    "        # out = tf.nn.leaky_relu(agg_neib + tf.multiply(x, agg_neib))\n",
    "        out_l = nn.LeakyReLU(negative_slope=0.2)(torch.matmul(x + agg_neib, self.weight_dict['W_att_l'].to(self.device)))\n",
    "        out_r = nn.LeakyReLU(negative_slope=0.2)(torch.matmul(torch.mul(x, agg_neib), self.weight_dict['W_att_r'].to(self.device)))\n",
    "        # out = tf.matmul(out, self.weights['W_att_trans']) + self.weights['b_att_trans']\n",
    "        out = out_l + out_r\n",
    "        return F.normalize(out,  p=2, dim=1)\n",
    "    \n",
    "    \n",
    "    def create_point_wise_loss(self, entity, pos_entity, neg_entity):\n",
    "        epsilon = 1e-7\n",
    "        # print('entity', entity.device)\n",
    "        # print('pos_entity', pos_entity.device)\n",
    "        # print('neg_entity', neg_entity.device)\n",
    "        \n",
    "        entity = torch.reshape(entity, (-1, 1, self.emb_size))\n",
    "        \n",
    "        # print(\"entity.shape\",entity.shape)\n",
    "        pos_entity = torch.reshape(pos_entity, (-1, self.weighted_sample_num, self.emb_size))\n",
    "        \n",
    "        \n",
    "        # print(\"pos_entity.shape\",pos_entity.shape)\n",
    "        pos_entity = torch.transpose(pos_entity, 2,1)\n",
    "        # print(\"pos_entity.shape\",pos_entity.shape)\n",
    "        pos_scores = torch.squeeze(torch.sum(torch.clamp(torch.log(nn.Sigmoid()(torch.matmul(entity, pos_entity)) + epsilon), -10, 0), 2))\n",
    "        # print(\"pos_scores.shape\",pos_scores.shape)\n",
    "        neg_entity = torch.reshape(neg_entity, (-1, self.weighted_sample_num, self.emb_size))\n",
    "        neg_entity = torch.transpose(neg_entity, 2,1)\n",
    "        neg_scores = torch.squeeze(torch.sum(torch.clamp(torch.log(nn.Sigmoid()(torch.matmul(entity, neg_entity)) + epsilon), -10, 0), 2))\n",
    "        # neg_scores = tf.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(entity, neg_entity), axis=1)) + epsilon)\n",
    "        # print('neg_scores', neg_scores.device)\n",
    "        # print('pos_scores', pos_scores.device)\n",
    "        return (-1*(torch.mean(pos_scores - neg_scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
