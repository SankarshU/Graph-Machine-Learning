{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "marine-logistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;43mCpp extension not loaded\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "from enum import Enum\n",
    "#from parse import parse_args\n",
    "import multiprocessing\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "#args = parse_args()\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "#import world\n",
    "#from world import cprint\n",
    "from time import time\n",
    "import torch\n",
    "#from dataloader import BasicDataset\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from torch import log\n",
    "#from dataloader import BasicDataset\n",
    "from time import time\n",
    "#from model import LightGCN\n",
    "#from model import PairWiseModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "#import utils\n",
    "#import dataloader\n",
    "from pprint import pprint\n",
    "#from utils import timer\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "#import model\n",
    "#import multiprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def cprint(words : str):\n",
    "    print(f\"\\033[0;30;43m{words}\\033[0m\")\n",
    "try:\n",
    "    from cppimport import imp_from_filepath\n",
    "    from os.path import join, dirname\n",
    "    path = join(dirname(__file__), \"sources/sampling.cpp\")\n",
    "    sampling = imp_from_filepath(path)\n",
    "    sampling.seed(world.seed)\n",
    "    sample_ext = True\n",
    "except:\n",
    "    cprint(\"Cpp extension not loaded\")\n",
    "    sample_ext = False\n",
    "\n",
    "#ROOT_PATH = \"/Users/gus/Desktop/light-gcn\"\n",
    "ROOT_PATH = \"/home/jupyter/NOCTrail/NGM/ngtt/PytorchGeometricTutorial/Tutorial6/LightGCN-PyTorch\"\n",
    "CODE_PATH = join(ROOT_PATH, 'code')\n",
    "DATA_PATH = join(ROOT_PATH, 'data')\n",
    "BOARD_PATH = join(CODE_PATH, 'runs')\n",
    "FILE_PATH = join(CODE_PATH, 'checkpoints')\n",
    "import sys\n",
    "sys.path.append(join(CODE_PATH, 'sources'))\n",
    "config ={'bpr_batch_size': 2048, 'latent_dim_rec': 64, 'lightGCN_n_layers': 3, 'dropout': 0, 'keep_prob': 0.6, 'A_n_fold': 100, 'test_u_batch_size': 100, 'multicore': 0, 'lr': 0.001, 'decay': 0.0001, 'pretrain': 0, 'A_split': False, 'bigdata': False}\n",
    "dataset=\"gowalla\"\n",
    "\n",
    "config ={'bpr_batch_size': 258, 'latent_dim_rec': 64, 'lightGCN_n_layers': 3, 'dropout': 0, 'keep_prob': 0.6, 'A_n_fold': 100, 'test_u_batch_size': 258, 'multicore': 0, 'lr': 0.001, 'decay': 0.0001, 'pretrain': 0, 'A_split': False, 'bigdata': False}\n",
    "\n",
    "#dataset=\"gowalla\"\n",
    "dataset=\"mtel\"\n",
    "\n",
    "GPU = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if GPU else \"cpu\")\n",
    "CORES = multiprocessing.cpu_count() // 2\n",
    "model='lgn'\n",
    "model_name=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "pleased-norwegian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/NOCTrail/NGM/ngtt/PytorchGeometricTutorial/Tutorial6/LightGCN-PyTorch/data\n"
     ]
    }
   ],
   "source": [
    "print(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "understanding-poultry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/NOCTrail/NGM/ngtt/PytorchGeometricTutorial/Tutorial6/LightGCN-PyTorch/code'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-ratio",
   "metadata": {},
   "source": [
    "#### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "smooth-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        print(\"init dataset\")\n",
    "    \n",
    "    @property\n",
    "    def n_users(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def m_items(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def trainDataSize(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def testDict(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def allPos(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserItemFeedback(self, users, items):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserPosItems(self, users):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserNegItems(self, users):\n",
    "        \"\"\"\n",
    "        not necessary for large dataset\n",
    "        it's stupid to return all neg items in super large dataset\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getSparseGraph(self):\n",
    "        \"\"\"\n",
    "        build a graph in torch.sparse.IntTensor.\n",
    "        Details in NGCF's matrix form\n",
    "        A = \n",
    "            |I,   R|\n",
    "            |R^T, I|\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "class Loader(BasicDataset):\n",
    "    \"\"\"\n",
    "    Dataset type for pytorch \\n\n",
    "    Incldue graph information\n",
    "    gowalla dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,config = config,path=\"../data/gowalla\"):\n",
    "        # train or test\n",
    "        cprint(f'loading [{path}]')\n",
    "        self.split = config['A_split']\n",
    "        self.folds = config['A_n_fold']\n",
    "        self.mode_dict = {'train': 0, \"test\": 1}\n",
    "        self.mode = self.mode_dict['train']\n",
    "        self.n_user = 0\n",
    "        self.m_item = 0\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "        self.path = path\n",
    "        trainUniqueUsers, trainItem, trainUser = [], [], []\n",
    "        testUniqueUsers, testItem, testUser = [], [], []\n",
    "        self.traindataSize = 0\n",
    "        self.testDataSize = 0\n",
    "\n",
    "        with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    trainUniqueUsers.append(uid)\n",
    "                    trainUser.extend([uid] * len(items))\n",
    "                    trainItem.extend(items)\n",
    "                    self.m_item = max(self.m_item, max(items))\n",
    "                    self.n_user = max(self.n_user, uid)\n",
    "                    self.traindataSize += len(items)\n",
    "        self.trainUniqueUsers = np.array(trainUniqueUsers)\n",
    "        self.trainUser = np.array(trainUser)\n",
    "        self.trainItem = np.array(trainItem)\n",
    "\n",
    "        with open(test_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    testUniqueUsers.append(uid)\n",
    "                    testUser.extend([uid] * len(items))\n",
    "                    testItem.extend(items)\n",
    "                    self.m_item = max(self.m_item, max(items))\n",
    "                    self.n_user = max(self.n_user, uid)\n",
    "                    self.testDataSize += len(items)\n",
    "        self.m_item += 1\n",
    "        self.n_user += 1\n",
    "        self.testUniqueUsers = np.array(testUniqueUsers)\n",
    "        self.testUser = np.array(testUser)\n",
    "        self.testItem = np.array(testItem)\n",
    "        \n",
    "        self.Graph = None\n",
    "        print(f\"{self.trainDataSize} interactions for training\")\n",
    "        print(f\"{self.testDataSize} interactions for testing\")\n",
    "        print(f\"{dataset} Sparsity : {(self.trainDataSize + self.testDataSize) / self.n_users / self.m_items}\")\n",
    "\n",
    "        # (users,items), bipartite graph\n",
    "        self.UserItemNet = csr_matrix((np.ones(len(self.trainUser)), (self.trainUser, self.trainItem)),\n",
    "                                      shape=(self.n_user, self.m_item))\n",
    "        self.users_D = np.array(self.UserItemNet.sum(axis=1)).squeeze()\n",
    "        self.users_D[self.users_D == 0.] = 1\n",
    "        self.items_D = np.array(self.UserItemNet.sum(axis=0)).squeeze()\n",
    "        self.items_D[self.items_D == 0.] = 1.\n",
    "        # pre-calculate\n",
    "        self._allPos = self.getUserPosItems(list(range(self.n_user)))\n",
    "        self.__testDict = self.__build_test()\n",
    "        #print('here',self.__testDict)\n",
    "        print(f\"{dataset} is ready to go\")\n",
    "\n",
    "    @property\n",
    "    def n_users(self):\n",
    "        return self.n_user\n",
    "    \n",
    "    @property\n",
    "    def m_items(self):\n",
    "        return self.m_item\n",
    "    \n",
    "    @property\n",
    "    def trainDataSize(self):\n",
    "        return self.traindataSize\n",
    "    \n",
    "    @property\n",
    "    def testDict(self):\n",
    "        return self.__testDict\n",
    "\n",
    "    @property\n",
    "    def allPos(self):\n",
    "        return self._allPos\n",
    "\n",
    "    def _split_A_hat(self,A):\n",
    "        A_fold = []\n",
    "        fold_len = (self.n_users + self.m_items) // self.folds\n",
    "        for i_fold in range(self.folds):\n",
    "            start = i_fold*fold_len\n",
    "            if i_fold == self.folds - 1:\n",
    "                end = self.n_users + self.m_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "            A_fold.append(self._convert_sp_mat_to_sp_tensor(A[start:end]).coalesce().to(world.device))\n",
    "        return A_fold\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        row = torch.Tensor(coo.row).long()\n",
    "        col = torch.Tensor(coo.col).long()\n",
    "        index = torch.stack([row, col])\n",
    "        data = torch.FloatTensor(coo.data)\n",
    "        return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n",
    "        \n",
    "    def getSparseGraph(self):\n",
    "        print(\"loading adjacency matrix\")\n",
    "        if self.Graph is None:\n",
    "            try:\n",
    "                pre_adj_mat = sp.load_npz(self.path + '/s_pre_adj_mat.npz')\n",
    "                print(\"successfully loaded...\")\n",
    "                norm_adj = pre_adj_mat\n",
    "            except :\n",
    "                print(\"generating adjacency matrix\")\n",
    "                s = time()\n",
    "                adj_mat = sp.dok_matrix((self.n_users + self.m_items, self.n_users + self.m_items), dtype=np.float32)\n",
    "                adj_mat = adj_mat.tolil()\n",
    "                R = self.UserItemNet.tolil()\n",
    "                adj_mat[:self.n_users, self.n_users:] = R\n",
    "                adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "                adj_mat = adj_mat.todok()\n",
    "                # adj_mat = adj_mat + sp.eye(adj_mat.shape[0])\n",
    "                \n",
    "                rowsum = np.array(adj_mat.sum(axis=1))\n",
    "                d_inv = np.power(rowsum, -0.5).flatten()\n",
    "                d_inv[np.isinf(d_inv)] = 0.\n",
    "                d_mat = sp.diags(d_inv)\n",
    "                \n",
    "                norm_adj = d_mat.dot(adj_mat)\n",
    "                norm_adj = norm_adj.dot(d_mat)\n",
    "                norm_adj = norm_adj.tocsr()\n",
    "                end = time()\n",
    "                print(f\"costing {end-s}s, saved norm_mat...\")\n",
    "                sp.save_npz(self.path + '/s_pre_adj_mat.npz', norm_adj)\n",
    "\n",
    "            if self.split == True:\n",
    "                self.Graph = self._split_A_hat(norm_adj)\n",
    "                print(\"done split matrix\")\n",
    "            else:\n",
    "                self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj)\n",
    "                self.Graph = self.Graph.coalesce().to(device)\n",
    "                print(\"don't split the matrix\")\n",
    "        return self.Graph\n",
    "\n",
    "    def __build_test(self):\n",
    "        \"\"\"\n",
    "        return:\n",
    "            dict: {user: [items]}\n",
    "        \"\"\"\n",
    "        test_data = {}\n",
    "        for i, item in enumerate(self.testItem):\n",
    "            user = self.testUser[i]\n",
    "            if test_data.get(user):\n",
    "                test_data[user].append(item)\n",
    "            else:\n",
    "                test_data[user] = [item]\n",
    "        return test_data\n",
    "\n",
    "    def getUserItemFeedback(self, users, items):\n",
    "        \"\"\"\n",
    "        users:\n",
    "            shape [-1]\n",
    "        items:\n",
    "            shape [-1]\n",
    "        return:\n",
    "            feedback [-1]\n",
    "        \"\"\"\n",
    "        # print(self.UserItemNet[users, items])\n",
    "        return np.array(self.UserItemNet[users, items]).astype('uint8').reshape((-1,))\n",
    "\n",
    "    def getUserPosItems(self, users):\n",
    "        posItems = []\n",
    "        for user in users:\n",
    "            posItems.append(self.UserItemNet[user].nonzero()[1])\n",
    "        return posItems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "possible-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def UniformSample_original(dataset, neg_ratio = 1):\n",
    "    dataset : BasicDataset\n",
    "    allPos = dataset.allPos\n",
    "    start = time()\n",
    "    if sample_ext:\n",
    "        S = sampling.sample_negative(dataset.n_users, dataset.m_items,\n",
    "                                     dataset.trainDataSize, allPos, neg_ratio)\n",
    "    else:\n",
    "        S = UniformSample_original_python(dataset)\n",
    "    return S\n",
    "\n",
    "def UniformSample_original_python(dataset):\n",
    "    \"\"\"\n",
    "    the original impliment of BPR Sampling in LightGCN\n",
    "    :return:\n",
    "        np.array\n",
    "    \"\"\"\n",
    "    total_start = time()\n",
    "    dataset : BasicDataset\n",
    "    user_num = dataset.trainDataSize\n",
    "    users = np.random.randint(0, dataset.n_users, user_num)\n",
    "    allPos = dataset.allPos\n",
    "    S = []\n",
    "    sample_time1 = 0.\n",
    "    sample_time2 = 0.\n",
    "    for i, user in enumerate(users):\n",
    "        start = time()\n",
    "        posForUser = allPos[user]\n",
    "        if len(posForUser) == 0:\n",
    "            continue\n",
    "        sample_time2 += time() - start\n",
    "        posindex = np.random.randint(0, len(posForUser))\n",
    "        positem = posForUser[posindex]\n",
    "        while True:\n",
    "            negitem = np.random.randint(0, dataset.m_items)\n",
    "            if negitem in posForUser:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        S.append([user, positem, negitem])\n",
    "        end = time()\n",
    "        sample_time1 += end - start\n",
    "    total = time() - total_start\n",
    "    return np.array(S)\n",
    "\n",
    "# ===================end samplers==========================\n",
    "# =====================utils====================================\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def getFileName():\n",
    "    if world.model_name == 'mf':\n",
    "        file = f\"mf-{world.dataset}-{world.config['latent_dim_rec']}.pth.tar\"\n",
    "    elif world.model_name == 'lgn':\n",
    "        file = f\"lgn-{world.dataset}-{world.config['lightGCN_n_layers']}-{world.config['latent_dim_rec']}.pth.tar\"\n",
    "    return os.path.join(world.FILE_PATH,file)\n",
    "\n",
    "def minibatch(*tensors, **kwargs):\n",
    "\n",
    "    batch_size = kwargs.get('batch_size', config['bpr_batch_size'])\n",
    "\n",
    "    if len(tensors) == 1:\n",
    "        tensor = tensors[0]\n",
    "        for i in range(0, len(tensor), batch_size):\n",
    "            yield tensor[i:i + batch_size]\n",
    "    else:\n",
    "        for i in range(0, len(tensors[0]), batch_size):\n",
    "            yield tuple(x[i:i + batch_size] for x in tensors)\n",
    "\n",
    "\n",
    "def shuffle(*arrays, **kwargs):\n",
    "\n",
    "    require_indices = kwargs.get('indices', False)\n",
    "\n",
    "    if len(set(len(x) for x in arrays)) != 1:\n",
    "        raise ValueError('All inputs to shuffle must have '\n",
    "                         'the same length.')\n",
    "\n",
    "    shuffle_indices = np.arange(len(arrays[0]))\n",
    "    np.random.shuffle(shuffle_indices)\n",
    "\n",
    "    if len(arrays) == 1:\n",
    "        result = arrays[0][shuffle_indices]\n",
    "    else:\n",
    "        result = tuple(x[shuffle_indices] for x in arrays)\n",
    "\n",
    "    if require_indices:\n",
    "        return result, shuffle_indices\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "class timer:\n",
    "    \"\"\"\n",
    "    Time context manager for code block\n",
    "        with timer():\n",
    "            do something\n",
    "        timer.get()\n",
    "    \"\"\"\n",
    "    from time import time\n",
    "    TAPE = [-1]  # global time record\n",
    "    NAMED_TAPE = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get():\n",
    "        if len(timer.TAPE) > 1:\n",
    "            return timer.TAPE.pop()\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    @staticmethod\n",
    "    def dict(select_keys=None):\n",
    "        hint = \"|\"\n",
    "        if select_keys is None:\n",
    "            for key, value in timer.NAMED_TAPE.items():\n",
    "                hint = hint + f\"{key}:{value:.2f}|\"\n",
    "        else:\n",
    "            for key in select_keys:\n",
    "                value = timer.NAMED_TAPE[key]\n",
    "                hint = hint + f\"{key}:{value:.2f}|\"\n",
    "        return hint\n",
    "\n",
    "    @staticmethod\n",
    "    def zero(select_keys=None):\n",
    "        if select_keys is None:\n",
    "            for key, value in timer.NAMED_TAPE.items():\n",
    "                timer.NAMED_TAPE[key] = 0\n",
    "        else:\n",
    "            for key in select_keys:\n",
    "                timer.NAMED_TAPE[key] = 0\n",
    "\n",
    "    def __init__(self, tape=None, **kwargs):\n",
    "        if kwargs.get('name'):\n",
    "            timer.NAMED_TAPE[kwargs['name']] = timer.NAMED_TAPE[\n",
    "                kwargs['name']] if timer.NAMED_TAPE.get(kwargs['name']) else 0.\n",
    "            self.named = kwargs['name']\n",
    "            if kwargs.get(\"group\"):\n",
    "                #TODO: add group function\n",
    "                pass\n",
    "        else:\n",
    "            self.named = False\n",
    "            self.tape = tape or timer.TAPE\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = timer.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.named:\n",
    "            timer.NAMED_TAPE[self.named] += timer.time() - self.start\n",
    "        else:\n",
    "            self.tape.append(timer.time() - self.start)\n",
    "\n",
    "\n",
    "# ====================Metrics==============================\n",
    "# =========================================================\n",
    "def RecallPrecision_ATk(test_data, r, k):\n",
    "    \"\"\"\n",
    "    test_data should be a list? cause users may have different amount of pos items. shape (test_batch, k)\n",
    "    pred_data : shape (test_batch, k) NOTE: pred_data should be pre-sorted\n",
    "    k : top-k\n",
    "    \"\"\"\n",
    "    right_pred = r[:, :k].sum(1)\n",
    "    precis_n = k\n",
    "    recall_n = np.array([len(test_data[i]) for i in range(len(test_data))])\n",
    "    recall = np.sum(right_pred/recall_n)\n",
    "    precis = np.sum(right_pred)/precis_n\n",
    "    return {'recall': recall, 'precision': precis}\n",
    "\n",
    "\n",
    "def MRRatK_r(r, k):\n",
    "    \"\"\"\n",
    "    Mean Reciprocal Rank\n",
    "    \"\"\"\n",
    "    pred_data = r[:, :k]\n",
    "    scores = np.log2(1./np.arange(1, k+1))\n",
    "    pred_data = pred_data/scores\n",
    "    pred_data = pred_data.sum(1)\n",
    "    return np.sum(pred_data)\n",
    "\n",
    "def NDCGatK_r(test_data,r,k):\n",
    "    \"\"\"\n",
    "    Normalized Discounted Cumulative Gain\n",
    "    rel_i = 1 or 0, so 2^{rel_i} - 1 = 1 or 0\n",
    "    \"\"\"\n",
    "    assert len(r) == len(test_data)\n",
    "    pred_data = r[:, :k]\n",
    "\n",
    "    test_matrix = np.zeros((len(pred_data), k))\n",
    "    for i, items in enumerate(test_data):\n",
    "        length = k if k <= len(items) else len(items)\n",
    "        test_matrix[i, :length] = 1\n",
    "    max_r = test_matrix\n",
    "    idcg = np.sum(max_r * 1./np.log2(np.arange(2, k + 2)), axis=1)\n",
    "    dcg = pred_data*(1./np.log2(np.arange(2, k + 2)))\n",
    "    dcg = np.sum(dcg, axis=1)\n",
    "    idcg[idcg == 0.] = 1.\n",
    "    ndcg = dcg/idcg\n",
    "    ndcg[np.isnan(ndcg)] = 0.\n",
    "    return np.sum(ndcg)\n",
    "\n",
    "def AUC(all_item_scores, dataset, test_data):\n",
    "    \"\"\"\n",
    "        design for a single user\n",
    "    \"\"\"\n",
    "    dataset : BasicDataset\n",
    "    r_all = np.zeros((dataset.m_items, ))\n",
    "    r_all[test_data] = 1\n",
    "    r = r_all[all_item_scores >= 0]\n",
    "    test_item_scores = all_item_scores[all_item_scores >= 0]\n",
    "    return roc_auc_score(r, test_item_scores)\n",
    "\n",
    "def getLabel(test_data, pred_data):\n",
    "    r = []\n",
    "    for i in range(len(test_data)):\n",
    "        groundTrue = test_data[i]\n",
    "        predictTopK = pred_data[i]\n",
    "        pred = list(map(lambda x: x in groundTrue, predictTopK))\n",
    "        pred = np.array(pred).astype(\"float\")\n",
    "        r.append(pred)\n",
    "    return np.array(r).astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "welcome-community",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;43mloading [../data/mtel_exp2]\u001b[0m\n",
      "2585 interactions for training\n",
      "988 interactions for testing\n",
      "mtel_exp2 Sparsity : 0.025410710475784085\n",
      "mtel_exp2 is ready to go\n"
     ]
    }
   ],
   "source": [
    "config ={'bpr_batch_size': 2048, 'latent_dim_rec': 64, 'lightGCN_n_layers': 3, 'dropout': 0, 'keep_prob': 0.6, 'A_n_fold': 100, 'test_u_batch_size': 100, 'multicore': 0, 'lr': 0.001, 'decay': 0.0001, 'pretrain': 0, 'A_split': False, 'bigdata': False}\n",
    "dataset=\"gowalla\"\n",
    "\n",
    "config ={'bpr_batch_size': 258, 'latent_dim_rec': 128, 'lightGCN_n_layers': 3, 'dropout': 0, 'keep_prob': 0.6, 'A_n_fold': 100, 'test_u_batch_size': 258, 'multicore': 0, 'lr': 0.001, 'decay': 0.0001, 'pretrain': 0, 'A_split': False, 'bigdata': False}\n",
    "lgcn_data1='/home/jupyter/NOCTrail/NGM/ngtt/PytorchGeometricTutorial/Tutorial6/LightGCN-PyTorch/data/mtel_exp2/'\n",
    "\n",
    "#dataset=\"gowalla\"\n",
    "dataset=\"mtel_exp2\"\n",
    "feature_vec=True\n",
    "GPU = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if GPU else \"cpu\")\n",
    "CORES = multiprocessing.cpu_count() // 2\n",
    "model='lgn'\n",
    "model_name=model\n",
    "if dataset in ['gowalla', 'yelp2018', 'amazon-book','mtel','mtel_exp2']:\n",
    "    dataset = Loader(path=\"../data/\"+dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "turkish-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.testDict ### how did this become a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "legislative-custom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['lightGCN_n_layers']\n",
    "dataset\n",
    "tar=np.load(lgcn_data1+'woarr.npy')\n",
    "tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "preceding-filing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([258, 128])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding_user.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "serious-budget",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16702/3136145756.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model.embedding_user.weight.shape\n",
    "\n",
    "\n",
    "w = list(model.parameters())[2]\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-color",
   "metadata": {},
   "source": [
    "#### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "alpine-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(BasicModel, self).__init__()\n",
    "    \n",
    "    def getUsersRating(self, users):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class LightGCN(BasicModel):\n",
    "    def __init__(self, \n",
    "                 config:dict, \n",
    "                 dataset:BasicDataset):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.config = config\n",
    "        self.dataset : dataloader.BasicDataset = dataset\n",
    "        self.__init_weight()\n",
    "\n",
    "    def __init_weight(self):\n",
    "        self.num_users  = self.dataset.n_users\n",
    "        self.num_items  = self.dataset.m_items\n",
    "        self.latent_dim = self.config['latent_dim_rec']\n",
    "        self.n_layers = self.config['lightGCN_n_layers']\n",
    "        self.keep_prob = self.config['keep_prob']\n",
    "        self.A_split = self.config['A_split']\n",
    "        self.embedding_user = torch.nn.Embedding(\n",
    "            num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
    "        self.embedding_item = torch.nn.Embedding(\n",
    "            num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
    "        if self.config['pretrain'] == -1:\n",
    "#             nn.init.xavier_uniform_(self.embedding_user.weight, gain=1)\n",
    "#             nn.init.xavier_uniform_(self.embedding_item.weight, gain=1)\n",
    "#             print('use xavier initilizer')\n",
    "# random normal init seems to be a better choice when lightGCN actually don't use any non-linear activation function\n",
    "            nn.init.normal_(self.embedding_user.weight, std=0.1)\n",
    "            nn.init.normal_(self.embedding_item.weight, std=0.1)\n",
    "            cprint('use NORMAL distribution initilizer')\n",
    "        elif feature_vec:\n",
    "            tar=np.load(lgcn_data1+'ttarr.npy')\n",
    "            war=np.load(lgcn_data1+'woarr.npy')\n",
    "            self.embedding_user.weight.data.copy_(torch.from_numpy(tar))\n",
    "            self.embedding_item.weight.data.copy_(torch.from_numpy(war))\n",
    "            print('use feature vector data for mtel')\n",
    "        else:\n",
    "            self.embedding_user.weight.data.copy_(torch.from_numpy(self.config['user_emb']))\n",
    "            self.embedding_item.weight.data.copy_(torch.from_numpy(self.config['item_emb']))\n",
    "            print('use pretarined data')\n",
    "        self.f = nn.Sigmoid()\n",
    "        self.Graph = self.dataset.getSparseGraph()\n",
    "        print(f\"lgn is already to go(dropout:{self.config['dropout']})\")\n",
    "\n",
    "        # print(\"save_txt\")\n",
    "    def __dropout_x(self, x, keep_prob):\n",
    "        size = x.size()\n",
    "        index = x.indices().t()\n",
    "        values = x.values()\n",
    "        random_index = torch.rand(len(values)) + keep_prob\n",
    "        random_index = random_index.int().bool()\n",
    "        index = index[random_index]\n",
    "        values = values[random_index]/keep_prob\n",
    "        g = torch.sparse.FloatTensor(index.t(), values, size)\n",
    "        return g\n",
    "    \n",
    "    def __dropout(self, keep_prob):\n",
    "        if self.A_split:\n",
    "            graph = []\n",
    "            for g in self.Graph:\n",
    "                graph.append(self.__dropout_x(g, keep_prob))\n",
    "        else:\n",
    "            graph = self.__dropout_x(self.Graph, keep_prob)\n",
    "        return graph\n",
    "    \n",
    "    def computer(self):\n",
    "        \"\"\"\n",
    "        propagate methods for lightGCN\n",
    "        \"\"\"       \n",
    "        users_emb = self.embedding_user.weight\n",
    "        items_emb = self.embedding_item.weight\n",
    "        all_emb = torch.cat([users_emb, items_emb])\n",
    "        #   torch.split(all_emb , [self.num_users, self.num_items])\n",
    "        \n",
    "        embs = [all_emb]\n",
    "        #print('Testpre',all_emb.size(),len(embs))\n",
    "        if self.config['dropout']:\n",
    "            if self.training:\n",
    "                print(\"droping\")\n",
    "                g_droped = self.__dropout(self.keep_prob)\n",
    "            else:\n",
    "                g_droped = self.Graph        \n",
    "        else:\n",
    "            g_droped = self.Graph    \n",
    "        \n",
    "        for layer in range(self.n_layers):\n",
    "            if self.A_split:\n",
    "                temp_emb = []\n",
    "                for f in range(len(g_droped)):\n",
    "                    temp_emb.append(torch.sparse.mm(g_droped[f], all_emb))\n",
    "                side_emb = torch.cat(temp_emb, dim=0)\n",
    "                all_emb = side_emb\n",
    "            else:\n",
    "                all_emb = torch.sparse.mm(g_droped, all_emb)\n",
    "            embs.append(all_emb)\n",
    "        embs = torch.stack(embs, dim=1)\n",
    "        #print('Test',embs.size())\n",
    "        light_out = torch.mean(embs, dim=1)\n",
    "        #print('Test3',light_out.size())\n",
    "        users, items = torch.split(light_out, [self.num_users, self.num_items])\n",
    "        return users, items\n",
    "    \n",
    "    def getUsersRating(self, users):\n",
    "        all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users.long()]\n",
    "        items_emb = all_items\n",
    "        rating = self.f(torch.matmul(users_emb, items_emb.t()))\n",
    "        return rating\n",
    "    \n",
    "    def getEmbedding(self, users, pos_items, neg_items):\n",
    "        all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users]\n",
    "        pos_emb = all_items[pos_items]\n",
    "        neg_emb = all_items[neg_items]\n",
    "        users_emb_ego = self.embedding_user(users)\n",
    "        pos_emb_ego = self.embedding_item(pos_items)\n",
    "        neg_emb_ego = self.embedding_item(neg_items)\n",
    "        return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego\n",
    "    \n",
    "    def bpr_loss(self, users, pos, neg):\n",
    "        (users_emb, pos_emb, neg_emb, \n",
    "        userEmb0,  posEmb0, negEmb0) = self.getEmbedding(users.long(), pos.long(), neg.long())\n",
    "        reg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n",
    "                         posEmb0.norm(2).pow(2)  +\n",
    "                         negEmb0.norm(2).pow(2))/float(len(users))\n",
    "        pos_scores = torch.mul(users_emb, pos_emb)\n",
    "        pos_scores = torch.sum(pos_scores, dim=1)\n",
    "        neg_scores = torch.mul(users_emb, neg_emb)\n",
    "        neg_scores = torch.sum(neg_scores, dim=1)\n",
    "        \n",
    "        loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n",
    "        \n",
    "        return loss, reg_loss\n",
    "       \n",
    "    def forward(self, users, items):\n",
    "        # compute embedding\n",
    "        #print('here,train')\n",
    "        all_users, all_items = self.computer()\n",
    "        # print('forward')\n",
    "        #all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users]\n",
    "        items_emb = all_items[items]\n",
    "        inner_pro = torch.mul(users_emb, items_emb)\n",
    "        gamma     = torch.sum(inner_pro, dim=1)\n",
    "        return gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "young-princeton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset.testDict\n",
    "#len(topks)\n",
    "#topks = eval(topks)\n",
    "len(topks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "vital-indicator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['test_u_batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "introductory-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_batch(X):\n",
    "    sorted_items = X[0].numpy()\n",
    "    groundTrue = X[1]\n",
    "    r = getLabel(groundTrue, sorted_items)\n",
    "    pre, recall, ndcg = [], [], []\n",
    "    for k in topks:\n",
    "        ret = RecallPrecision_ATk(groundTrue, r, k)\n",
    "        pre.append(ret['precision'])\n",
    "        recall.append(ret['recall'])\n",
    "        ndcg.append(NDCGatK_r(groundTrue,r,k))\n",
    "    return {'recall':np.array(recall), \n",
    "            'precision':np.array(pre), \n",
    "            'ndcg':np.array(ndcg)}\n",
    "        \n",
    "            \n",
    "def Test(dataset, Recmodel, epoch, w=None, multicore=0):\n",
    "    u_batch_size = config['test_u_batch_size']\n",
    "    u_batch_size = 25\n",
    "    dataset: BasicDataset\n",
    "    #testDict: dict = testDict\n",
    "    testDict=dataset.testDict\n",
    "    Recmodel: LightGCN\n",
    "    # eval mode with no dropout\n",
    "    Recmodel = Recmodel.eval()\n",
    "    topks=eval('[20]')\n",
    "    max_K = max(topks)\n",
    "    #max_K = 20\n",
    "    if multicore == 1:\n",
    "        pool = multiprocessing.Pool(CORES)\n",
    "    results = {'precision': np.zeros(len(topks)),\n",
    "               'recall': np.zeros(len(topks)),\n",
    "               'ndcg': np.zeros(len(topks))}\n",
    "    with torch.no_grad():\n",
    "        users = list(testDict.keys())\n",
    "        try:\n",
    "            assert u_batch_size <= len(users) / 10\n",
    "        except AssertionError:\n",
    "            print(f\"test_u_batch_size is too big for this dataset, try a small one {len(users) // 10}\")\n",
    "        users_list = []\n",
    "        rating_list = []\n",
    "        groundTrue_list = []\n",
    "        # auc_record = []\n",
    "        # ratings = []\n",
    "        total_batch = len(users) // u_batch_size + 1\n",
    "        for batch_users in minibatch(users, batch_size=u_batch_size):\n",
    "            allPos = dataset.getUserPosItems(batch_users)\n",
    "            groundTrue = [testDict[u] for u in batch_users]\n",
    "            batch_users_gpu = torch.Tensor(batch_users).long()\n",
    "            batch_users_gpu = batch_users_gpu.to(device)\n",
    "\n",
    "            rating = Recmodel.getUsersRating(batch_users_gpu)\n",
    "            #rating = rating.cpu()\n",
    "            exclude_index = []\n",
    "            exclude_items = []\n",
    "            for range_i, items in enumerate(allPos):\n",
    "                exclude_index.extend([range_i] * len(items))\n",
    "                exclude_items.extend(items)\n",
    "            rating[exclude_index, exclude_items] = -(1<<10)\n",
    "            _, rating_K = torch.topk(rating, k=max_K)\n",
    "            rating = rating.cpu().numpy()\n",
    "            # aucs = [ \n",
    "            #         utils.AUC(rating[i],\n",
    "            #                   dataset, \n",
    "            #                   test_data) for i, test_data in enumerate(groundTrue)\n",
    "            #     ]\n",
    "            # auc_record.extend(aucs)\n",
    "            del rating\n",
    "            users_list.append(batch_users)\n",
    "            rating_list.append(rating_K.cpu())\n",
    "            groundTrue_list.append(groundTrue)\n",
    "        assert total_batch == len(users_list)\n",
    "        X = zip(rating_list, groundTrue_list)\n",
    "        if multicore == 1:\n",
    "            pre_results = pool.map(test_one_batch, X)\n",
    "        else:\n",
    "            pre_results = []\n",
    "            for x in X:\n",
    "                pre_results.append(test_one_batch(x))\n",
    "        scale = float(u_batch_size/len(users))\n",
    "        for result in pre_results:\n",
    "            results['recall'] += result['recall']\n",
    "            results['precision'] += result['precision']\n",
    "            results['ndcg'] += result['ndcg']\n",
    "        results['recall'] /= float(len(users))\n",
    "        results['precision'] /= float(len(users))\n",
    "        results['ndcg'] /= float(len(users))\n",
    "        # results['auc'] = np.mean(auc_record)\n",
    "#         if world.tensorboard:\n",
    "#             w.add_scalars(f'Test/Recall@{world.topks}',\n",
    "#                           {str(world.topks[i]): results['recall'][i] for i in range(len(world.topks))}, epoch)\n",
    "#             w.add_scalars(f'Test/Precision@{world.topks}',\n",
    "#                           {str(world.topks[i]): results['precision'][i] for i in range(len(world.topks))}, epoch)\n",
    "#             w.add_scalars(f'Test/NDCG@{world.topks}',\n",
    "#                           {str(world.topks[i]): results['ndcg'][i] for i in range(len(world.topks))}, epoch)\n",
    "        if multicore == 1:\n",
    "            pool.close()\n",
    "        print(results)\n",
    "        return results\n",
    "def trainX(users, pos, neg):\n",
    "    lr = config['lr']\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    weight_decay=config['decay']\n",
    "    model.train()\n",
    "    #model1.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss, reg_loss = model.bpr_loss(users, pos, neg)\n",
    "    reg_loss = reg_loss*weight_decay\n",
    "    loss = loss + reg_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "environmental-dubai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use pretarined data for mtel\n",
      "loading adjacency matrix\n",
      "lgn is already to go(dropout:0)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_epochs=200\n",
    "tensorboard=False\n",
    "\n",
    "Neg_k = 1\n",
    "#try:\n",
    "w = None\n",
    "model = LightGCN(config, dataset)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "three-coaching",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4725774824619293\n",
      "1 0.45334097472104157\n",
      "2 0.4162073623050343\n",
      "3 0.40990275144577026\n",
      "4 0.36578974669629877\n",
      "5 0.35280039635571564\n",
      "6 0.3111469420519742\n",
      "7 0.28081268072128296\n",
      "8 0.2500784464857795\n",
      "9 0.22313375093720175\n",
      "10 0.2058591056953777\n",
      "11 0.17229273237965323\n",
      "12 0.15753097967668014\n",
      "13 0.133743592961268\n",
      "14 0.11639016731218858\n",
      "15 0.09621852568604729\n",
      "16 0.09026293321089311\n",
      "17 0.08132583851164038\n",
      "18 0.0665747147392143\n",
      "19 0.06818115914409811\n",
      "20 0.06055733493783257\n",
      "21 0.05834843048995191\n",
      "22 0.04589585680514574\n",
      "23 0.03874898964369839\n",
      "24 0.03962222622199492\n",
      "25 0.03679925406521017\n",
      "26 0.034364225681532516\n",
      "27 0.03674066049808806\n",
      "28 0.027208494699814102\n",
      "29 0.02717580519277941\n",
      "30 0.027402342974462292\n",
      "31 0.024431638123298235\n",
      "32 0.03124790300022472\n",
      "33 0.01962466969747435\n",
      "34 0.017775047570466995\n",
      "35 0.019586289470846004\n",
      "36 0.04041233387860385\n",
      "37 0.020515251142734833\n",
      "38 0.019571549936451695\n",
      "39 0.01688308766196397\n",
      "40 0.019234930419109085\n",
      "41 0.016303093003278427\n",
      "42 0.016689984530041165\n",
      "43 0.020011770877648483\n",
      "44 0.014610933690247211\n",
      "45 0.016151172257113187\n",
      "46 0.012973618862981146\n",
      "47 0.015187591187317263\n",
      "48 0.014779935347508977\n",
      "49 0.010404121405868367\n",
      "50 0.01170242518525232\n",
      "51 0.013668321454050865\n",
      "52 0.012222611662847075\n",
      "53 0.009612186610783365\n",
      "54 0.014103840782561085\n",
      "55 0.009246556299992582\n",
      "56 0.012267222204668955\n",
      "57 0.015034731435166164\n",
      "58 0.01303341510620984\n",
      "59 0.013212243814698675\n",
      "60 0.0069365910974077205\n",
      "61 0.011808418160812422\n",
      "62 0.008559863464060154\n",
      "63 0.009284902522763745\n",
      "64 0.009303159069862555\n",
      "65 0.00790301723067056\n",
      "66 0.00996694842946123\n",
      "67 0.012661740734157238\n",
      "68 0.01498450696553019\n",
      "69 0.008589435527524487\n",
      "70 0.009264370373619551\n",
      "71 0.0065101216208528385\n",
      "72 0.012164832960644906\n",
      "73 0.009276201969689944\n",
      "74 0.011406795206395063\n",
      "75 0.00981445464474911\n",
      "76 0.010349036363715475\n",
      "77 0.010549130680208857\n",
      "78 0.010887172739868138\n",
      "79 0.010264964583753184\n",
      "80 0.005636703254739669\n",
      "81 0.01011711547405205\n",
      "82 0.0074511412742801686\n",
      "83 0.00790663876316764\n",
      "84 0.0100035218691284\n",
      "85 0.010139420289884914\n",
      "86 0.008982718456536531\n",
      "87 0.012159480914388869\n",
      "88 0.008561845710077747\n",
      "89 0.007026644296605478\n",
      "90 0.014057099946181883\n",
      "91 0.0076425766403024845\n",
      "92 0.013269881760193543\n",
      "93 0.008071529446169734\n",
      "94 0.00875508994795382\n",
      "95 0.0064257716611874375\n",
      "96 0.00897851715457033\n",
      "97 0.0058207821989939975\n",
      "98 0.026908497580073097\n",
      "99 0.010430871339684183\n",
      "100 0.008642528248442844\n",
      "101 0.007453598526560448\n",
      "102 0.008593764875761488\n",
      "103 0.010367164674045687\n",
      "104 0.007972237103703346\n",
      "105 0.014214687155220996\n",
      "106 0.008369983470236713\n",
      "107 0.0091581071134318\n",
      "108 0.009865757628259334\n",
      "109 0.006791237455962057\n",
      "110 0.010006025306541811\n",
      "111 0.01078928088430654\n",
      "112 0.008437386819754134\n",
      "113 0.006018102370117876\n",
      "114 0.008060318608344956\n",
      "115 0.00926532278853384\n",
      "116 0.008275795579803262\n",
      "117 0.008957501162182201\n",
      "118 0.010839795185761019\n",
      "119 0.008069953193295409\n",
      "120 0.008590444647283717\n",
      "121 0.00896210710264065\n",
      "122 0.007415068039501255\n",
      "123 0.0074718413154848595\n",
      "124 0.0055713148533620615\n",
      "125 0.010641144842586735\n",
      "126 0.008483918838795613\n",
      "127 0.009163059251890942\n",
      "128 0.008390354585241188\n",
      "129 0.008682248637672852\n",
      "130 0.007801553657786412\n",
      "131 0.009325516685335473\n",
      "132 0.009176457120867615\n",
      "133 0.006885692667724056\n",
      "134 0.00922778412826698\n",
      "135 0.006144194472157819\n",
      "136 0.008403810100968589\n",
      "137 0.009890903136692941\n",
      "138 0.008545977804301814\n",
      "139 0.008222700943323698\n",
      "140 0.009459411132742058\n",
      "141 0.006737888388505036\n",
      "142 0.0069299159486862745\n",
      "143 0.00796155466444113\n",
      "144 0.007770780929025601\n",
      "145 0.008155195021324536\n",
      "146 0.008497628214007074\n",
      "147 0.00784465903416276\n",
      "148 0.006309761729260737\n",
      "149 0.007498288251967592\n",
      "150 0.012617945057255301\n",
      "151 0.009108964216218075\n",
      "152 0.007084216996604068\n",
      "153 0.008161146363074129\n",
      "154 0.007366033059290864\n",
      "155 0.008006925271316008\n",
      "156 0.005816744534637441\n",
      "157 0.007380658411420882\n",
      "158 0.008212814446200024\n",
      "159 0.00750378518238325\n",
      "160 0.00968348259233277\n",
      "161 0.007978023738938977\n",
      "162 0.011194767123511569\n",
      "163 0.00819752283859998\n",
      "164 0.007209415547549725\n",
      "165 0.005723786286332391\n",
      "166 0.005290696206925945\n",
      "167 0.0061348981054669075\n",
      "168 0.0070757927076721735\n",
      "169 0.007606980327347463\n",
      "170 0.007581263039769096\n",
      "171 0.010200264165177941\n",
      "172 0.0057623371989889574\n",
      "173 0.007538106144321236\n",
      "174 0.009622632451778785\n",
      "175 0.006561093042943288\n",
      "176 0.00529994989152659\n",
      "177 0.0051069906912744045\n",
      "178 0.004499701592563229\n",
      "179 0.007183723744343628\n",
      "180 0.007148195608434352\n",
      "181 0.008329522372646765\n",
      "182 0.007717693108133972\n",
      "183 0.007234048098325729\n",
      "184 0.007815274103036658\n",
      "185 0.009419774678959087\n",
      "186 0.009847564506344497\n",
      "187 0.0053899053759364915\n",
      "188 0.00693391845561564\n",
      "189 0.006039100614461032\n",
      "190 0.005224961290051314\n",
      "191 0.004635493641465225\n",
      "192 0.0067455118035220285\n",
      "193 0.009877316621978853\n",
      "194 0.006731542546979405\n",
      "195 0.008992539295418695\n",
      "196 0.00721997031095353\n",
      "197 0.009458853164687753\n",
      "198 0.006042564012618227\n",
      "199 0.00803189600860192\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(TRAIN_epochs):\n",
    "    S = UniformSample_original(dataset)\n",
    "    users = torch.Tensor(S[:, 0]).long()\n",
    "    posItems = torch.Tensor(S[:, 1]).long()\n",
    "    negItems = torch.Tensor(S[:, 2]).long()\n",
    "    users = users.to(device)\n",
    "    posItems = posItems.to(device)\n",
    "    negItems = negItems.to(device)\n",
    "    users, posItems, negItems = shuffle(users, posItems, negItems)\n",
    "    total_batch = len(users) // config['bpr_batch_size'] + 1\n",
    "    aver_loss = 0.\n",
    "    for (batch_i,\n",
    "        (batch_users,\n",
    "        batch_pos,\n",
    "        batch_neg)) in enumerate(minibatch(users,\n",
    "                                                       posItems,\n",
    "                                                       negItems,\n",
    "                                                       batch_size=config['bpr_batch_size'])):\n",
    "        cri = trainX(batch_users, batch_pos, batch_neg)\n",
    "        #print(cri,'cri')\n",
    "        aver_loss += cri\n",
    "    aver_loss = aver_loss / total_batch\n",
    "    print(epoch,aver_loss)\n",
    "\n",
    "#Test(dataset, Recmodel, epoch, w, world.config['multicore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "objective-travel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 0.00803189600860192\n"
     ]
    }
   ],
   "source": [
    "#topks=eval('[20]')\n",
    "print(epoch,aver_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "buried-discrimination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': array([0.11627907]), 'recall': array([0.54744021]), 'ndcg': array([0.40401321])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': array([0.11627907]),\n",
       " 'recall': array([0.54744021]),\n",
       " 'ndcg': array([0.40401321])}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test(dataset, model, epoch, w=None, multicore=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "legislative-society",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testpre torch.Size([70839, 64]) 1\n",
      "Test torch.Size([70839, 4, 64])\n",
      "Test3 torch.Size([70839, 64])\n"
     ]
    }
   ],
   "source": [
    "u_batch_size = config['test_u_batch_size']\n",
    "dataset: BasicDataset\n",
    "testDict=dataset.testDict\n",
    "model: LightGCN\n",
    "    # eval mode with no dropout\n",
    "model = model.eval()\n",
    "topks=eval('[20]')\n",
    "max_K = max(topks)\n",
    "    #max_K = 20\n",
    "#     if multicore == 1:\n",
    "#         pool = multiprocessing.Pool(CORES)\n",
    "results = {'precision': np.zeros(len(topks)),\n",
    "               'recall': np.zeros(len(topks)),\n",
    "               'ndcg': np.zeros(len(topks))}\n",
    "with torch.no_grad():\n",
    "    users = list(testDict.keys())\n",
    "    users_list = []\n",
    "    rating_list = []\n",
    "    groundTrue_list = []\n",
    "        # auc_record = []\n",
    "        # ratings = []\n",
    "    total_batch = len(users) // u_batch_size + 1\n",
    "    for batch_users in minibatch(users, batch_size=u_batch_size):\n",
    "        #print(batch_users)\n",
    "        allPos = dataset.getUserPosItems(batch_users)\n",
    "        groundTrue = [testDict[u] for u in batch_users]\n",
    "        batch_users_gpu = torch.Tensor(batch_users).long()\n",
    "        batch_users_gpu = batch_users_gpu.to(device)\n",
    "        rating = model.getUsersRating(batch_users_gpu)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "express-burst",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 40981])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "voluntary-office",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "regular-character",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([258, 64])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding_user.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "miniature-belize",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40981"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.m_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-intermediate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-pygx]",
   "language": "python",
   "name": "conda-env-anaconda3-pygx-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
